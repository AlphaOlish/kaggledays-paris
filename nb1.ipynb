{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb \n",
    "\n",
    "base_path = \"../input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "navigation = pd.read_csv(os.path.join(base_path, 'navigation.csv'))\n",
    "sales = pd.read_csv(os.path.join(base_path, 'sales.csv'))\n",
    "train = pd.read_csv(os.path.join(base_path, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(base_path, 'test.csv'))\n",
    "vimages = pd.read_csv(os.path.join(base_path, 'vimages.csv'))\n",
    "sub = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave-one-out target encoding for different colors of the same product\n",
    "\n",
    "product_descriptor = ['product_type', 'product_gender', 'macro_function', \n",
    "                      'function', 'sub_function', 'model', 'aesthetic_sub_line', 'macro_material',\n",
    "                      'month']\n",
    "\n",
    "product_target_sum = train.groupby(product_descriptor)['target'].sum().reset_index(name = 'sum_target')\n",
    "product_target_count = train.groupby(product_descriptor)['target'].count().reset_index(name = 'count_target')\n",
    "product_target_stats = pd.merge(product_target_sum, product_target_count, on = product_descriptor)\n",
    "\n",
    "train = train.merge(product_target_stats, on = product_descriptor, how = 'left')\n",
    "test = test.merge(product_target_stats, on = product_descriptor, how = 'left')\n",
    "\n",
    "train['mean_target'] = (train['sum_target'] - train['target'])/(train['count_target']-1)\n",
    "test['mean_target'] = (test['sum_target'])/(test['count_target'])\n",
    "\n",
    "train.drop(['sum_target','count_target'],axis=1,inplace=True)\n",
    "test.drop(['sum_target','count_target'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts for categorical features in train+test\n",
    "\n",
    "count_vec_cols = ['macro_function', 'function', 'sub_function', 'model',\n",
    "                  'aesthetic_sub_line', 'macro_material', 'color']\n",
    "\n",
    "for col in count_vec_cols:\n",
    "    tmp = pd.DataFrame({'sku_hash': pd.concat([train['sku_hash'],test['sku_hash']]), \n",
    "                        col:pd.concat([train[col],test[col]])})\n",
    "    tmp = pd.DataFrame(tmp.groupby(col)['sku_hash'].count()).reset_index()\n",
    "    tmp.columns = [col,col+'_count']\n",
    "\n",
    "    train = train.merge(tmp, on = col, how = 'left')\n",
    "    test = test.merge(tmp, on = col, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of page views by different traffic source\n",
    "\n",
    "traffic_source_views = navigation.groupby(['sku_hash','traffic_source'])['page_views'].sum().reset_index()\n",
    "traffic_source_views = traffic_source_views.pivot(index='sku_hash', columns='traffic_source', values='page_views').reset_index()\n",
    "traffic_source_views.columns = ['sku_hash', \n",
    "                                'page_views_nav1', 'page_views_nav2', 'page_views_nav3', \n",
    "                                'page_views_nav4', 'page_views_nav5', 'page_views_nav6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of sales by different type\n",
    "\n",
    "type_sales = sales.groupby(['sku_hash','type'])['sales_quantity'].sum().reset_index()\n",
    "type_sales = type_sales.pivot(index='sku_hash', columns='type', values='sales_quantity').reset_index()\n",
    "type_sales.columns = ['sku_hash', 'sales_quantity_type1', 'sales_quantity_type2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of sales by different zone\n",
    "\n",
    "zone_sales = sales.groupby(['sku_hash','zone_number'])['sales_quantity'].sum().reset_index()\n",
    "zone_sales = zone_sales.pivot(index='sku_hash', columns='zone_number', values='sales_quantity').reset_index()\n",
    "zone_sales.columns = ['sku_hash', \n",
    "                      'sales_quantity_zone1', 'sales_quantity_zone2', 'sales_quantity_zone3', \n",
    "                      'sales_quantity_zone4', 'sales_quantity_zone5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall stats of sales, page views  and twitter sentiments\n",
    "\n",
    "navigation_stats = navigation.groupby('sku_hash')['page_views'].sum().reset_index(name='page_views')\n",
    "sales_stats = sales.groupby('sku_hash')['sales_quantity','TotalBuzzPost', 'TotalBuzz',\n",
    "       'NetSentiment', 'PositiveSentiment', 'NegativeSentiment', 'Impressions'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cross validation splits\n",
    "\n",
    "train['idx'] = pd.Categorical(train.sku_hash).codes\n",
    "train['idx'] = train['idx'] % 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge everything for train\n",
    "\n",
    "X = train.copy()\n",
    "X = X.merge(navigation_stats, on = 'sku_hash', how = 'left')\n",
    "X = X.merge(sales_stats, on = 'sku_hash', how = 'left')\n",
    "X = X.merge(traffic_source_views, on = 'sku_hash', how = 'left')\n",
    "X = X.merge(type_sales, on = 'sku_hash', how = 'left')\n",
    "X = X.merge(zone_sales, on = 'sku_hash', how = 'left')\n",
    "\n",
    "X.loc[X.product_type=='Accessories','product_type'] = '0'\n",
    "X.loc[X.product_type=='Leather Goods','product_type'] = '1'\n",
    "X.product_type = X.product_type.astype(int)\n",
    "\n",
    "X.loc[X.product_gender=='Women','product_gender'] = '-1'\n",
    "X.loc[X.product_gender=='Unisex','product_gender'] = '0'\n",
    "X.loc[X.product_gender=='Men','product_gender'] = '1'\n",
    "X.product_gender = X.product_gender.astype(int)\n",
    "\n",
    "# transform label to meet the metric\n",
    "X['y'] = np.log(X['target'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge everything for test \n",
    "\n",
    "Z = test.copy()\n",
    "Z = Z.merge(navigation_stats, on = 'sku_hash', how = 'left')\n",
    "Z = Z.merge(sales_stats, on = 'sku_hash', how = 'left')\n",
    "Z = Z.merge(traffic_source_views, on = 'sku_hash', how = 'left')\n",
    "Z = Z.merge(type_sales, on = 'sku_hash', how = 'left')\n",
    "Z = Z.merge(zone_sales, on = 'sku_hash', how = 'left')\n",
    "\n",
    "Z.loc[Z.product_type=='Accessories','product_type'] = '0'\n",
    "Z.loc[Z.product_type=='Leather Goods','product_type'] = '1'\n",
    "Z.product_type = Z.product_type.astype(int)\n",
    "\n",
    "Z.loc[Z.product_gender=='Women','product_gender'] = '-1'\n",
    "Z.loc[Z.product_gender=='Unisex','product_gender'] = '0'\n",
    "Z.loc[Z.product_gender=='Men','product_gender'] = '1'\n",
    "Z.product_gender = Z.product_gender.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['product_type', 'product_gender', \n",
    "            'page_views', 'sales_quantity',\n",
    "            'TotalBuzzPost', 'TotalBuzz', 'NetSentiment', 'PositiveSentiment', 'NegativeSentiment', 'Impressions',\n",
    "            'fr_FR_price',\n",
    "            'macro_function_count', 'function_count', 'sub_function_count', 'model_count', 'aesthetic_sub_line_count', 'macro_material_count', 'color_count',\n",
    "            'page_views_nav1', 'page_views_nav2', 'page_views_nav3', 'page_views_nav4', 'page_views_nav5', 'page_views_nav6',\n",
    "            'sales_quantity_type1', 'sales_quantity_type2',\n",
    "            'sales_quantity_zone1','sales_quantity_zone2','sales_quantity_zone3', 'sales_quantity_zone4','sales_quantity_zone5',\n",
    "            'mean_target',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate xgboost objects for a specific month\n",
    "\n",
    "def train_test_split(tr, te, mo, feats, num_folds):\n",
    "    \n",
    "    Xtrain = []\n",
    "    ytrain = []\n",
    "    dtrain = []\n",
    "    Xval = []\n",
    "    yval = []\n",
    "    dval = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        \n",
    "        Xtrain.append(tr.loc[(tr.month == mo) & (tr.idx != i), feats].values)\n",
    "        ytrain.append(tr.loc[(tr.month == mo) & (tr.idx != i), 'y'].values)\n",
    "        dtrain.append(xgb.DMatrix(Xtrain[i],ytrain[i]))\n",
    "        \n",
    "        Xval.append(tr.loc[(tr.month == mo) & (tr.idx == i), feats].values)\n",
    "        yval.append(tr.loc[(tr.month == mo) & (tr.idx == i), 'y'].values)\n",
    "        dval.append(xgb.DMatrix(Xval[i],yval[i]))\n",
    "\n",
    "    Xtest = te.loc[(te.month == mo),feats].values    \n",
    "    dtest = xgb.DMatrix(Xtest)\n",
    "    \n",
    "    return dtrain, dval, dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define xgboost parameters to use in models\n",
    "\n",
    "param = {} \n",
    "param['objective'] = 'reg:linear'\n",
    "param['eval_metric'] =  'rmse'\n",
    "param['booster'] = 'gbtree'\n",
    "param['eta'] = 0.025\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['num_parallel_tree'] = 3\n",
    "param['min_child_weight'] = 25\n",
    "param['gamma'] = 5\n",
    "param['max_depth'] =  3\n",
    "param['silent'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models for the 1 month\n",
    "\n",
    "dtrain, dval, dtest = train_test_split(tr = X, te = Z, mo = 1, feats = features, num_folds = 5)\n",
    "\n",
    "model_m1 = []\n",
    "for i in range(5):\n",
    "    model_m1.append(\n",
    "        xgb.train(\n",
    "                  param,\n",
    "                  dtrain[i],\n",
    "                  50000,\n",
    "                  [(dtrain[i],'train'), (dval[i],'eval')],\n",
    "                  early_stopping_rounds = 200,\n",
    "                  verbose_eval = False)\n",
    "    )\n",
    "    \n",
    "# run predictions for the 1 month    \n",
    "    \n",
    "oof_m1 = []\n",
    "oof_test_m1 = []\n",
    "for i in range(5):\n",
    "    oof_m1.append(model_m1[i].predict(dval[i]))\n",
    "    oof_test_m1.append(model_m1[i].predict(dtest))\n",
    "    \n",
    "test_m1 = np.mean(oof_test_m1, axis=0)    \n",
    "    \n",
    "m1 = {}\n",
    "for i in range(5):\n",
    "    m1 = {**m1, **dict(zip(X.loc[(X.month==1) & (X.idx==i),'sku_hash'], oof_m1[i]))}\n",
    "    \n",
    "m1 = {**m1, **dict(zip(Z.loc[(Z.month==1),'sku_hash'], test_m1))}\n",
    "    \n",
    "oof_m1 = pd.DataFrame.from_dict(m1, orient='index').reset_index()    \n",
    "oof_m1.columns = ['sku_hash', 'oof_m1']\n",
    "\n",
    "X2 = pd.merge(X.copy(), oof_m1, on = 'sku_hash')\n",
    "Z2 = pd.merge(Z.copy(), oof_m1, on = 'sku_hash')\n",
    "features2 = features + ['oof_m1']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models for the 2 month\n",
    "\n",
    "dtrain2, dval2, dtest2 = train_test_split(tr = X2, te = Z2, mo = 2, feats = features2, num_folds = 5)\n",
    "\n",
    "model_m2 = []\n",
    "\n",
    "for i in range(5):\n",
    "    model_m2.append(\n",
    "        xgb.train(\n",
    "                  param,\n",
    "                  dtrain2[i],\n",
    "                  50000,\n",
    "                  [(dtrain2[i],'train'), (dval2[i],'eval')],\n",
    "                  early_stopping_rounds = 200,\n",
    "                  verbose_eval = False)\n",
    "    )\n",
    "\n",
    "# run predictions for the 2 month        \n",
    "    \n",
    "oof_m2 = []\n",
    "oof_test_m2 = []\n",
    "for i in range(5):\n",
    "    oof_m2.append(model_m2[i].predict(dval2[i]))\n",
    "    oof_test_m2.append(model_m2[i].predict(dtest2))\n",
    "    \n",
    "test_m2 = np.mean(oof_test_m2, axis=0)    \n",
    "    \n",
    "m2 = {}\n",
    "for i in range(5):\n",
    "    m2 = {**m2, **dict(zip(X.loc[(X.month==2) & (X.idx==i),'sku_hash'], oof_m2[i]))}\n",
    "    \n",
    "m2 = {**m2, **dict(zip(Z.loc[(Z.month==2),'sku_hash'], test_m2))}\n",
    "    \n",
    "oof_m2 = pd.DataFrame.from_dict(m2, orient='index').reset_index()    \n",
    "oof_m2.columns = ['sku_hash', 'oof_m2']\n",
    "\n",
    "X3 = pd.merge(X2.copy(), oof_m2, on = 'sku_hash')\n",
    "Z3 = pd.merge(Z2.copy(), oof_m2, on = 'sku_hash')\n",
    "features3 = features2 + ['oof_m2']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models for the 3 month\n",
    "\n",
    "dtrain3, dval3, dtest3 = train_test_split(tr = X3, te = Z3, mo = 3, feats = features3, num_folds = 5)\n",
    "\n",
    "model_m3 = []\n",
    "\n",
    "for i in range(5):\n",
    "    model_m3.append(\n",
    "        xgb.train(\n",
    "                  param,\n",
    "                  dtrain3[i],\n",
    "                  50000,\n",
    "                  [(dtrain3[i],'train'),(dval3[i],'eval')],\n",
    "                  early_stopping_rounds = 200,\n",
    "                  verbose_eval = False)\n",
    "    )\n",
    "\n",
    "# run predictions for the 3 month        \n",
    "    \n",
    "oof_m3 = []\n",
    "oof_test_m3 = []\n",
    "for i in range(5):\n",
    "    oof_m3.append(model_m3[i].predict(dval3[i]))\n",
    "    oof_test_m3.append(model_m3[i].predict(dtest3))\n",
    "    \n",
    "test_m3 = np.mean(oof_test_m3, axis=0)    \n",
    "    \n",
    "m3 = {}\n",
    "for i in range(5):\n",
    "    m3 = {**m3, **dict(zip(X.loc[(X.month==3) & (X.idx==i),'sku_hash'], oof_m3[i]))}\n",
    "    \n",
    "m3 = {**m3, **dict(zip(Z.loc[(Z.month==3),'sku_hash'], test_m3))}\n",
    "    \n",
    "oof_m3 = pd.DataFrame.from_dict(m3, orient='index').reset_index()    \n",
    "oof_m3.columns = ['sku_hash', 'oof_m3']\n",
    "\n",
    "X3 = pd.merge(X3.copy(), oof_m3, on = 'sku_hash')\n",
    "Z3 = pd.merge(Z3.copy(), oof_m3, on = 'sku_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single vector of predictions for both train and test\n",
    "\n",
    "Z3['target'] = 0\n",
    "Z3.loc[Z3.month == 1, 'target'] = Z3.loc[Z3.month == 1, 'oof_m1'] \n",
    "Z3.loc[Z3.month == 2, 'target'] = Z3.loc[Z3.month == 2, 'oof_m2'] \n",
    "Z3.loc[Z3.month == 3, 'target'] = Z3.loc[Z3.month == 3, 'oof_m3'] \n",
    "\n",
    "X3['pred_target'] = 0\n",
    "X3.loc[X3.month == 1, 'pred_target'] = X3.loc[X3.month == 1, 'oof_m1'] \n",
    "X3.loc[X3.month == 2, 'pred_target'] = X3.loc[X3.month == 2, 'oof_m2'] \n",
    "X3.loc[X3.month == 3, 'pred_target'] = X3.loc[X3.month == 3, 'oof_m3'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month1: 0.44243199303177\n",
      "month2: 0.5833878625335227\n",
      "month3: 0.7168724677773026\n",
      "overall: 0.5916061757809871\n"
     ]
    }
   ],
   "source": [
    "# some cross validation diagnostics\n",
    "\n",
    "print(f\"month1: {np.sqrt(np.mean((X3.loc[X3.month==1,'y'] - X3.loc[X3.month==1,'pred_target'])**2))}\")\n",
    "print(f\"month2: {np.sqrt(np.mean((X3.loc[X3.month==2,'y'] - X3.loc[X3.month==2,'pred_target'])**2))}\")\n",
    "print(f\"month3: {np.sqrt(np.mean((X3.loc[X3.month==3,'y'] - X3.loc[X3.month==3,'pred_target'])**2))}\")\n",
    "print(f\"overall: {np.sqrt(np.mean((X3['y'] - X3['pred_target'])**2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a submission\n",
    "\n",
    "Z3['target'] = np.exp(Z3.target)-1\n",
    "final_sub = Z3[['ID','target']]\n",
    "final_sub.to_csv(os.path.join(base_path,'silly-raddar-sub4.csv'),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a oof train version of a submision\n",
    "\n",
    "X3['target'] = np.exp(X3.pred_target)-1\n",
    "cv_sub = X3[['ID','target']]\n",
    "cv_sub.to_csv(os.path.join(base_path,'silly-raddar-cv4.csv'),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export features for anokas to use\n",
    "\n",
    "X3[['ID']+features].to_csv(os.path.join(base_path,'raddar-features-train.csv'),index=None)\n",
    "Z3[['ID']+features].to_csv(os.path.join(base_path,'raddar-features-test.csv'),index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
